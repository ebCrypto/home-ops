{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"### My home operations repository :octocat: _... managed with Flux, Renovate and GitHub Actions_ \ud83e\udd16 [![Discord](https://img.shields.io/discord/673534664354430999?style=for-the-badge&label&logo=discord&logoColor=white&color=blue)](https://discord.gg/k8s-at-home) [![Kubernetes](https://img.shields.io/badge/v1.24-blue?style=for-the-badge&logo=kubernetes&logoColor=white)](https://k3s.io/) [![Pre-commit](https://img.shields.io/badge/pre--commit-enabled-blue?logo=pre-commit&logoColor=white&label&style=for-the-badge)](https://github.com/pre-commit/pre-commit) [![Renovate](https://img.shields.io/github/workflow/status/onedr0p/home-ops/Schedule%20-%20Renovate?label=&logo=renovatebot&style=for-the-badge&color=blue)](https://github.com/onedr0p/home-ops/actions/workflows/schedule-renovate.yaml) [![LoC](https://img.shields.io/tokei/lines/github/onedr0p/home-ops?style=for-the-badge&color=blue&label&logo=codefactor&logoColor=white)](https://github.com/onedr0p/home-ops/graphs/contributors) [![Home-Internet](https://img.shields.io/uptimerobot/status/m784591389-ddbc4c84041a70eb6f6a3fb4?color=brightgreeen&label=Home%20Internet&style=for-the-badge&logo=opnSense&logoColor=white)](https://uptimerobot.com) [![Plex](https://img.shields.io/uptimerobot/status/m784591338-cbf3205bc18109108eb0ea8e?logo=plex&logoColor=white&color=brightgreeen&label=Plex&style=for-the-badge)](https://plex.tv) [![Home-Assistant](https://img.shields.io/uptimerobot/status/m786203807-32ce99612d7b2d01b89c4315?logo=homeassistant&logoColor=white&color=brightgreeen&label=Home%20Assistant&style=for-the-badge)](https://www.home-assistant.io/) [![Grafana](https://img.shields.io/uptimerobot/status/m792427620-04fcdd7089a84863ec9f398d?logo=grafana&logoColor=white&color=brightgreeen&label=Grafana&style=for-the-badge)](https://www.grafana.com/) \ud83d\udcd6 Overview \u00b6 This is a mono repository for my home infrastructure and Kubernetes cluster. I try to adhere to Infrastructure as Code (IaC) and GitOps practices using the tools like Ansible , Terraform , Kubernetes , Flux , Renovate and GitHub Actions . \u26f5 Kubernetes \u00b6 There's an excellent template over at onedr0p/flux-cluster-template if you wanted to try and follow along with some of the practices I use here. Installation \u00b6 My cluster is k3s provisioned overtop bare-metal Fedora Server using the Ansible galaxy role ansible-role-k3s . This is a semi hyper-converged cluster, workloads and block storage are sharing the same available resources on my nodes while I have a separate server for (NFS) file storage. \ud83d\udd38 Click here to see my Ansible playbooks and roles. Core Components \u00b6 projectcalico/calico : Internal Kubernetes networking plugin. rook/rook : Distributed block storage for peristent storage. mozilla/sops : Manages secrets for Kubernetes, Ansible and Terraform. kubernetes-sigs/external-dns : Automatically manages DNS records from my cluster in a cloud DNS provider. jetstack/cert-manager : Creates SSL certificates for services in my Kubernetes cluster. kubernetes/ingress-nginx : Ingress controller to expose HTTP traffic to pods over DNS. GitOps \u00b6 Flux watches my cluster folder (see Directories below) and makes the changes to my cluster based on the YAML manifests. Renovate watches my entire repository looking for dependency updates, when they are found a PR is automatically created. When some PRs are merged Flux applies the changes to my cluster. Directories \u00b6 This Git repository contains the following directories ( kustomizatons ) under cluster . \ud83d\udcc1 cluster # k8s cluster defined as code \u251c\u2500\ud83d\udcc1 flux # flux, gitops operator, loaded before everything \u251c\u2500\ud83d\udcc1 crds # custom resources, loaded before \ud83d\udcc1 core and \ud83d\udcc1 apps \u251c\u2500\ud83d\udcc1 charts # helm repos, loaded before \ud83d\udcc1 core and \ud83d\udcc1 apps \u251c\u2500\ud83d\udcc1 config # cluster config, loaded before \ud83d\udcc1 core and \ud83d\udcc1 apps \u251c\u2500\ud83d\udcc1 core # crucial apps, namespaced dir tree, loaded before \ud83d\udcc1 apps \u2514\u2500\ud83d\udcc1 apps # regular apps, categorized dir tree, loaded last Networking \u00b6 Name CIDR Kubernetes Nodes 192.168.42.0/24 Kubernetes external services (Calico w/ BGP) 192.168.69.0/24 Kubernetes pods 10.42.0.0/16 Kubernetes services 10.43.0.0/16 HAProxy configured on Opnsense for the Kubernetes Control Plane Load Balancer. Calico configured with externalIPs to expose Kubernetes services with their own IP over BGP which is configured on my router. Data Backup and Recovery \u00b6 Due to issues, restrictions or nuances with Velero , Benji , Gemini , Kasten K10 by Veeam , Stash by AppsCode and others I am currently using a DIY (or more specifically a \"Poor Man's Backup\") solution that is leveraging Kyverno , Kopia and native Kubernetes CronJob and Job resources. At a high level the way this operates is that: Kyverno creates a CronJob for each PersistentVolumeClaim resource that contain a label of snapshot.home.arpa/enabled: \"true\" Everyday the CronJob creates a Job and uses Kopia to connect to a Kopia repository on my NAS over NFS and then snapshots the contents of the app data mount into the Kopia repository The snapshots made by Kopia are incremental which makes the Job run very quick. The app data mount is frozen during backup to prevent writes and unfrozen when the snapshot is complete. The PersistentVolumeClaim resources must contain the labels app.kubernetes.io/name , app.kubernetes.io/instance , and snapshot.home.arpa/enabled Some important notes on the implementation of this method: Kopia has a Web UI which you can deploy into your cluster to have access to the repository via the UI or by executing into the Pod and using the Kopia CLI. This deployment is required if using the Taskfile snapshot:create and snapshot:restore tasks I created. Recovery is done manually by using a different Job which utilizes a task with Taskfile I wrote a task that creates a restore Job that shutdowns the app and restores a snapshot from the Kopia repository into the apps' data PersistentVolumeClaim and then puts the app back into a running state There is another CronJob that syncs the Kopia repository to Backblaze B2 everyday. \ud83c\udf10 DNS \u00b6 Ingress Controller \u00b6 Over WAN, I have port forwarded ports 80 and 443 to the load balancer IP of my ingress controller that's running in my Kubernetes cluster. Cloudflare works as a proxy to hide my homes WAN IP and also as a firewall. When not on my home network, all the traffic coming into my ingress controller on port 80 and 443 comes from Cloudflare. In Opnsense I block all IPs not originating from the Cloudflares list of IP ranges . \ud83d\udd38 Cloudflare is also configured to GeoIP block all countries except a few I have whitelisted Internal DNS \u00b6 k8s_gateway is deployed on Opnsense . With this setup, k8s_gateway has direct access to my clusters ingress records and serves DNS for them in my internal network. k8s_gateway is only listening on 127.0.0.1 on port 53 . For adblocking, I have AdGuard Home also deployed on Opnsense which has a upstream server pointing the k8s_gateway I mentioned above. Adguard Home listens on my MANAGEMENT , SERVER , IOT and GUEST networks on port 53 . In my firewall rules I have NAT port redirection forcing all the networks to use the Adguard Home DNS server. Without much engineering of DNS @home, these options have made my Opnsense router a single point of failure for DNS. I believe this is ok though because my router should have the most uptime of all my systems. External DNS \u00b6 external-dns is deployed in my cluster and configure to sync DNS records to Cloudflare . The only ingresses external-dns looks at to gather DNS records to put in Cloudflare are ones that I explicitly set an annotation of external-dns.home.arpa/enabled: \"true\" \ud83d\udd38 Click here to see how else I manage Cloudflare with Terraform. Dynamic DNS \u00b6 My home IP can change at any given time and in order to keep my WAN IP address up to date on Cloudflare. I have deployed a CronJob in my cluster, this periodically checks and updates the A record ipv4.domain.tld . \ud83d\udd27 Hardware \u00b6 Click to see da rack! Device Count OS Disk Size Data Disk Size Ram Operating System Purpose Protectli FW6D 1 500GB mSATA N/A 16GB Opnsense 22 Router Intel NUC8i3BEK 3 256GB NVMe N/A 32GB Ubuntu 22.04 Kubernetes (k3s) Masters Intel NUC8i5BEH 3 240GB SSD 1TB NVMe (rook-ceph) 64GB Ubuntu 22.04 Kubernetes (k3s) Workers PowerEdge T340 1 2TB SSD 8x12TB ZFS RAIDz2 64GB Ubuntu 22.04 Apps (Minio, Nexus, etc) & NFS Lenovo SA120 1 N/A 8x12TB N/A N/A DAS Raspberry Pi 1 32GB SD Card N/A 4GB PiKVM Network KVM TESmart 8 Port KVM Switch 1 N/A N/A N/A N/A Network KVM switch for PiKVM APC SMT1500RM2U w/ NIC 1 N/A N/A N/A N/A UPS CyberPower PDU41001 2 N/A N/A N/A N/A PDU \ud83e\udd1d Graditude and Thanks \u00b6 Thanks to all the people who donate their time to the Kubernetes @Home community. A lot of inspiration for my cluster comes from the people that have shared their clusters with the k8s-at-home GitHub topic. \ud83d\udcdc Changelog \u00b6 See commit history \ud83d\udd0f License \u00b6 See LICENSE","title":"Index"},{"location":"#overview","text":"This is a mono repository for my home infrastructure and Kubernetes cluster. I try to adhere to Infrastructure as Code (IaC) and GitOps practices using the tools like Ansible , Terraform , Kubernetes , Flux , Renovate and GitHub Actions .","title":"\ud83d\udcd6 Overview"},{"location":"#kubernetes","text":"There's an excellent template over at onedr0p/flux-cluster-template if you wanted to try and follow along with some of the practices I use here.","title":"\u26f5 Kubernetes"},{"location":"#installation","text":"My cluster is k3s provisioned overtop bare-metal Fedora Server using the Ansible galaxy role ansible-role-k3s . This is a semi hyper-converged cluster, workloads and block storage are sharing the same available resources on my nodes while I have a separate server for (NFS) file storage. \ud83d\udd38 Click here to see my Ansible playbooks and roles.","title":"Installation"},{"location":"#core-components","text":"projectcalico/calico : Internal Kubernetes networking plugin. rook/rook : Distributed block storage for peristent storage. mozilla/sops : Manages secrets for Kubernetes, Ansible and Terraform. kubernetes-sigs/external-dns : Automatically manages DNS records from my cluster in a cloud DNS provider. jetstack/cert-manager : Creates SSL certificates for services in my Kubernetes cluster. kubernetes/ingress-nginx : Ingress controller to expose HTTP traffic to pods over DNS.","title":"Core Components"},{"location":"#gitops","text":"Flux watches my cluster folder (see Directories below) and makes the changes to my cluster based on the YAML manifests. Renovate watches my entire repository looking for dependency updates, when they are found a PR is automatically created. When some PRs are merged Flux applies the changes to my cluster.","title":"GitOps"},{"location":"#directories","text":"This Git repository contains the following directories ( kustomizatons ) under cluster . \ud83d\udcc1 cluster # k8s cluster defined as code \u251c\u2500\ud83d\udcc1 flux # flux, gitops operator, loaded before everything \u251c\u2500\ud83d\udcc1 crds # custom resources, loaded before \ud83d\udcc1 core and \ud83d\udcc1 apps \u251c\u2500\ud83d\udcc1 charts # helm repos, loaded before \ud83d\udcc1 core and \ud83d\udcc1 apps \u251c\u2500\ud83d\udcc1 config # cluster config, loaded before \ud83d\udcc1 core and \ud83d\udcc1 apps \u251c\u2500\ud83d\udcc1 core # crucial apps, namespaced dir tree, loaded before \ud83d\udcc1 apps \u2514\u2500\ud83d\udcc1 apps # regular apps, categorized dir tree, loaded last","title":"Directories"},{"location":"#networking","text":"Name CIDR Kubernetes Nodes 192.168.42.0/24 Kubernetes external services (Calico w/ BGP) 192.168.69.0/24 Kubernetes pods 10.42.0.0/16 Kubernetes services 10.43.0.0/16 HAProxy configured on Opnsense for the Kubernetes Control Plane Load Balancer. Calico configured with externalIPs to expose Kubernetes services with their own IP over BGP which is configured on my router.","title":"Networking"},{"location":"#data-backup-and-recovery","text":"Due to issues, restrictions or nuances with Velero , Benji , Gemini , Kasten K10 by Veeam , Stash by AppsCode and others I am currently using a DIY (or more specifically a \"Poor Man's Backup\") solution that is leveraging Kyverno , Kopia and native Kubernetes CronJob and Job resources. At a high level the way this operates is that: Kyverno creates a CronJob for each PersistentVolumeClaim resource that contain a label of snapshot.home.arpa/enabled: \"true\" Everyday the CronJob creates a Job and uses Kopia to connect to a Kopia repository on my NAS over NFS and then snapshots the contents of the app data mount into the Kopia repository The snapshots made by Kopia are incremental which makes the Job run very quick. The app data mount is frozen during backup to prevent writes and unfrozen when the snapshot is complete. The PersistentVolumeClaim resources must contain the labels app.kubernetes.io/name , app.kubernetes.io/instance , and snapshot.home.arpa/enabled Some important notes on the implementation of this method: Kopia has a Web UI which you can deploy into your cluster to have access to the repository via the UI or by executing into the Pod and using the Kopia CLI. This deployment is required if using the Taskfile snapshot:create and snapshot:restore tasks I created. Recovery is done manually by using a different Job which utilizes a task with Taskfile I wrote a task that creates a restore Job that shutdowns the app and restores a snapshot from the Kopia repository into the apps' data PersistentVolumeClaim and then puts the app back into a running state There is another CronJob that syncs the Kopia repository to Backblaze B2 everyday.","title":"Data Backup and Recovery"},{"location":"#dns","text":"","title":"\ud83c\udf10 DNS"},{"location":"#ingress-controller","text":"Over WAN, I have port forwarded ports 80 and 443 to the load balancer IP of my ingress controller that's running in my Kubernetes cluster. Cloudflare works as a proxy to hide my homes WAN IP and also as a firewall. When not on my home network, all the traffic coming into my ingress controller on port 80 and 443 comes from Cloudflare. In Opnsense I block all IPs not originating from the Cloudflares list of IP ranges . \ud83d\udd38 Cloudflare is also configured to GeoIP block all countries except a few I have whitelisted","title":"Ingress Controller"},{"location":"#internal-dns","text":"k8s_gateway is deployed on Opnsense . With this setup, k8s_gateway has direct access to my clusters ingress records and serves DNS for them in my internal network. k8s_gateway is only listening on 127.0.0.1 on port 53 . For adblocking, I have AdGuard Home also deployed on Opnsense which has a upstream server pointing the k8s_gateway I mentioned above. Adguard Home listens on my MANAGEMENT , SERVER , IOT and GUEST networks on port 53 . In my firewall rules I have NAT port redirection forcing all the networks to use the Adguard Home DNS server. Without much engineering of DNS @home, these options have made my Opnsense router a single point of failure for DNS. I believe this is ok though because my router should have the most uptime of all my systems.","title":"Internal DNS"},{"location":"#external-dns","text":"external-dns is deployed in my cluster and configure to sync DNS records to Cloudflare . The only ingresses external-dns looks at to gather DNS records to put in Cloudflare are ones that I explicitly set an annotation of external-dns.home.arpa/enabled: \"true\" \ud83d\udd38 Click here to see how else I manage Cloudflare with Terraform.","title":"External DNS"},{"location":"#dynamic-dns","text":"My home IP can change at any given time and in order to keep my WAN IP address up to date on Cloudflare. I have deployed a CronJob in my cluster, this periodically checks and updates the A record ipv4.domain.tld .","title":"Dynamic DNS"},{"location":"#hardware","text":"Click to see da rack! Device Count OS Disk Size Data Disk Size Ram Operating System Purpose Protectli FW6D 1 500GB mSATA N/A 16GB Opnsense 22 Router Intel NUC8i3BEK 3 256GB NVMe N/A 32GB Ubuntu 22.04 Kubernetes (k3s) Masters Intel NUC8i5BEH 3 240GB SSD 1TB NVMe (rook-ceph) 64GB Ubuntu 22.04 Kubernetes (k3s) Workers PowerEdge T340 1 2TB SSD 8x12TB ZFS RAIDz2 64GB Ubuntu 22.04 Apps (Minio, Nexus, etc) & NFS Lenovo SA120 1 N/A 8x12TB N/A N/A DAS Raspberry Pi 1 32GB SD Card N/A 4GB PiKVM Network KVM TESmart 8 Port KVM Switch 1 N/A N/A N/A N/A Network KVM switch for PiKVM APC SMT1500RM2U w/ NIC 1 N/A N/A N/A N/A UPS CyberPower PDU41001 2 N/A N/A N/A N/A PDU","title":"\ud83d\udd27 Hardware"},{"location":"#graditude-and-thanks","text":"Thanks to all the people who donate their time to the Kubernetes @Home community. A lot of inspiration for my cluster comes from the people that have shared their clusters with the k8s-at-home GitHub topic.","title":"\ud83e\udd1d Graditude and Thanks"},{"location":"#changelog","text":"See commit history","title":"\ud83d\udcdc Changelog"},{"location":"#license","text":"See LICENSE","title":"\ud83d\udd0f License"},{"location":"apps/Authentication/","text":"Authentication \u00b6 GLAuth \u00b6 Repo configuration \u00b6 Add/Update .vscode/extensions.json { \"files.associations\" : { \"**/cluster/**/*.sops.toml\" : \"plaintext\" } } Add/Update .gitattributes *.sops.toml linguist-language=JSON Add/Update .sops.yaml - path_regex : cluster/.*\\.sops\\.toml key_groups : - age : - age15uzrw396e67z9wdzsxzdk7ka0g2gr3l460e0slaea563zll3hdfqwqxdta App Configuration \u00b6 Below are the decrypted versions of the sops encrypted toml files. passbcrypt can be generated at https://gchq.github.io/CyberChef/#recipe=Bcrypt(12)To_Hex(%27None%27,0) server.sops.toml debug = true [ldap] enabled = true listen = \"0.0.0.0:389\" [ldaps] enabled = false [api] enabled = true tls = false listen = \"0.0.0.0:5555\" [backend] datastore = \"config\" baseDN = \"dc=home,dc=arpa\" groups.sops.toml [[groups]] name = \"svcaccts\" gidnumber = 6500 [[groups]] name = \"admins\" gidnumber = 6501 [[groups]] name = \"people\" gidnumber = 6502 users.sops.toml [[users]] name = \"search\" uidnumber = 5000 primarygroup = 6500 passbcrypt = \"\" [[users.capabilities]] action = \"search\" object = \"*\" [[users]] name = \"devin\" mail = \"\" givenname = \"Devin\" sn = \"Buhl\" uidnumber = 5001 primarygroup = 6502 othergroups = [ 6501 ] passbcrypt = \"\" [[users]] name = \"louie\" mail = \"\" givenname = \"Louie\" sn = \"Buhl\" uidnumber = 5002 primarygroup = 6502 passbcrypt = \"\"","title":"Authentication"},{"location":"apps/Authentication/#authentication","text":"","title":"Authentication"},{"location":"apps/Authentication/#glauth","text":"","title":"GLAuth"},{"location":"apps/Authentication/#repo-configuration","text":"Add/Update .vscode/extensions.json { \"files.associations\" : { \"**/cluster/**/*.sops.toml\" : \"plaintext\" } } Add/Update .gitattributes *.sops.toml linguist-language=JSON Add/Update .sops.yaml - path_regex : cluster/.*\\.sops\\.toml key_groups : - age : - age15uzrw396e67z9wdzsxzdk7ka0g2gr3l460e0slaea563zll3hdfqwqxdta","title":"Repo configuration"},{"location":"apps/Authentication/#app-configuration","text":"Below are the decrypted versions of the sops encrypted toml files. passbcrypt can be generated at https://gchq.github.io/CyberChef/#recipe=Bcrypt(12)To_Hex(%27None%27,0) server.sops.toml debug = true [ldap] enabled = true listen = \"0.0.0.0:389\" [ldaps] enabled = false [api] enabled = true tls = false listen = \"0.0.0.0:5555\" [backend] datastore = \"config\" baseDN = \"dc=home,dc=arpa\" groups.sops.toml [[groups]] name = \"svcaccts\" gidnumber = 6500 [[groups]] name = \"admins\" gidnumber = 6501 [[groups]] name = \"people\" gidnumber = 6502 users.sops.toml [[users]] name = \"search\" uidnumber = 5000 primarygroup = 6500 passbcrypt = \"\" [[users.capabilities]] action = \"search\" object = \"*\" [[users]] name = \"devin\" mail = \"\" givenname = \"Devin\" sn = \"Buhl\" uidnumber = 5001 primarygroup = 6502 othergroups = [ 6501 ] passbcrypt = \"\" [[users]] name = \"louie\" mail = \"\" givenname = \"Louie\" sn = \"Buhl\" uidnumber = 5002 primarygroup = 6502 passbcrypt = \"\"","title":"App Configuration"},{"location":"apps/Databases/","text":"Databases \u00b6 Postgres \u00b6 S3 Configuration \u00b6 Create ~/.mc/config.json { \"version\" : \"10\" , \"aliases\" : { \"minio\" : { \"url\" : \"https://s3.<domain>\" , \"accessKey\" : \"<access-key>\" , \"secretKey\" : \"<secret-key>\" , \"api\" : \"S3v4\" , \"path\" : \"auto\" } } } Create the outline user and password mc admin user add minio postgresql <super-secret-password> Create the outline bucket mc mb minio/postgresql Create postgresql-user-policy.json { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Action\" : [ \"s3:ListBucket\" , \"s3:PutObject\" , \"s3:GetObject\" , \"s3:DeleteObject\" ], \"Effect\" : \"Allow\" , \"Resource\" : [ \"arn:aws:s3:::postgresql/*\" , \"arn:aws:s3:::postgresql\" ], \"Sid\" : \"\" } ] } Apply the bucket policies mc admin policy add minio postgresql-private postgresql-user-policy.json Associate private policy with the user mc admin policy set minio postgresql-private user = postgresql","title":"Databases"},{"location":"apps/Databases/#databases","text":"","title":"Databases"},{"location":"apps/Databases/#postgres","text":"","title":"Postgres"},{"location":"apps/Databases/#s3-configuration","text":"Create ~/.mc/config.json { \"version\" : \"10\" , \"aliases\" : { \"minio\" : { \"url\" : \"https://s3.<domain>\" , \"accessKey\" : \"<access-key>\" , \"secretKey\" : \"<secret-key>\" , \"api\" : \"S3v4\" , \"path\" : \"auto\" } } } Create the outline user and password mc admin user add minio postgresql <super-secret-password> Create the outline bucket mc mb minio/postgresql Create postgresql-user-policy.json { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Action\" : [ \"s3:ListBucket\" , \"s3:PutObject\" , \"s3:GetObject\" , \"s3:DeleteObject\" ], \"Effect\" : \"Allow\" , \"Resource\" : [ \"arn:aws:s3:::postgresql/*\" , \"arn:aws:s3:::postgresql\" ], \"Sid\" : \"\" } ] } Apply the bucket policies mc admin policy add minio postgresql-private postgresql-user-policy.json Associate private policy with the user mc admin policy set minio postgresql-private user = postgresql","title":"S3 Configuration"}]}